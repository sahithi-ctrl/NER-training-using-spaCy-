{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ipykernel\n",
    "! pip install -U pip setuptools wheel\n",
    "! pip install -U spacy[transformers, lookups]==3.0.3\n",
    "! python -m spacy download en_core_web_trf\n",
    "! pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio==0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch lts.html\n",
    "! pip install cupy-cuda113\n",
    "! pip install scispacy\n",
    "! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_scibert-0.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b55d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import time\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "# Load the CSV data\n",
    "data = pd.read_csv('https://github.com/uml-digital-health/Labs/blob/main/Project3/data/train.csv')\n",
    "\n",
    "# Convert the CSV data into training examples\n",
    "examples = []\n",
    "for _, row in data.iterrows():\n",
    "    start, end = int(row['start']), int(row['end'])\n",
    "    entity_type = row['sbdh']\n",
    "    text = row['text']\n",
    "    example = Example.from_dict(\n",
    "        spacy.blank(\"en\"), \n",
    "        {\"text\": text, \"entities\": [(start, end, entity_type)]}\n",
    "    )\n",
    "    examples.append(example)\n",
    "\n",
    "# Load the scibert model\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"scibert\", config={\"model\": \"scibert_scivocab_uncased\"})\n",
    "\n",
    "# Add the entity labels\n",
    "entity_labels = set([example['entities'][0][2] for example in examples])\n",
    "for label in entity_labels:\n",
    "    nlp.entity.add_label(label)\n",
    "\n",
    "# Train the model\n",
    "n_iter = 100\n",
    "batch_size = 4\n",
    "nlp.begin_training()\n",
    "for i in range(n_iter):\n",
    "    losses = {}\n",
    "    random.shuffle(examples)\n",
    "    batches = minibatch(examples, size=compounding(batch_size, batch_size*2, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, drop=0.5, losses=losses)\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk('my_ner_model')\n",
    "\n",
    "# Use the model to process input text and generate NER tags\n",
    "doc = nlp(\"Some example text that contains a tobacco-related behavior.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2014b42b",
   "metadata": {},
   "source": [
    "TO EVALUATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the trained model\n",
    "nlp = spacy.load('my_ner_model')\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(docs):\n",
    "    start_time = time.time()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for doc in docs:\n",
    "        true_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        doc = nlp(doc.text)\n",
    "        pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        true_labels.extend(true_ents)\n",
    "        pred_labels.extend(pred_ents)\n",
    "    # Calculate the evaluation metrics\n",
    "    strict_metrics = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "    partial_metrics = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "    # Calculate the processing time\n",
    "    processing_time = (time.time() - start_time) / len(docs)\n",
    "    return strict_metrics, partial_metrics, processing_time\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('https://github.com/uml-digital-health/Labs/blob/main/Project3/data/test.csv')\n",
    "\n",
    "# Convert the test data into Doc objects\n",
    "docs = [Doc(nlp.vocab, text=row['text']) for _, row in test_data.iterrows()]\n",
    "\n",
    "# Evaluate the model\n",
    "strict_metrics, partial_metrics, processing_time = evaluate_model(docs)\n",
    "\n",
    "# Print the evaluation results\n",
    "print('Strict matching:\\nPrecision: {}\\nRecall: {}\\nF1 score: {}'.format(strict_metrics[0], strict_metrics[1], strict_metrics[2]))\n",
    "print('Partial matching:\\nPrecision: {}\\nRecall: {}\\nF1 score: {}'.format(partial_metrics[0], partial_metrics[1], partial_metrics[2]))\n",
    "print('Processing time per document: {} seconds'.format(processing_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
